{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\at once\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\at once\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\at once\\anaconda3\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\at once\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(list(api.info()['models'].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "model=api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(target_word, model, topn=5):\n",
    "    similar_words = model.most_similar(positive=[target_word], topn=topn)\n",
    "    return similar_words, [target_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target word: science\n",
      "Top predictions:\n",
      "sciences: 0.8548\n",
      "research: 0.8437\n",
      "institute: 0.8386\n",
      "studies: 0.8369\n",
      "physics: 0.8314\n"
     ]
    }
   ],
   "source": [
    "predictions, word_used = skipgram(\"science\", model)\n",
    "\n",
    "print(f\"Target word: {word_used[0]}\")\n",
    "print(\"Top predictions:\")\n",
    "for word, score in predictions:\n",
    "    print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_predict(context_words, model, topn=5):\n",
    "    context_vector = []\n",
    "    valid_words = []\n",
    "\n",
    "    for word in context_words:\n",
    "        if word in model.key_to_index:\n",
    "            context_vector.append(model[word])\n",
    "            valid_words.append(word)\n",
    "\n",
    "    if not context_vector:\n",
    "        return [], []\n",
    "\n",
    "    mean_vector = np.mean(context_vector, axis=0)\n",
    "\n",
    "    similar_words = model.most_similar(positive=[mean_vector], topn=topn)\n",
    "\n",
    "    return similar_words, valid_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used context words: ['sciences', 'research', 'institute', 'studies', 'physics']\n",
      "Top predictions:\n",
      "science: 0.9228\n"
     ]
    }
   ],
   "source": [
    "context = [\"sciences\",\"research\",\"institute\",\"studies\",\"physics\"]\n",
    "predictions, used_words = cbow_predict(context, model)\n",
    "\n",
    "print(\"Used context words:\", used_words)\n",
    "print(\"Top predictions:\")\n",
    "for word, score in predictions:\n",
    "    if word not in used_words:\n",
    "        print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used context words: ['messi', 'maradona', 'neymar', 'forlan', 'aguero']\n",
      "Top predictions:\n",
      "figo: 0.8558\n",
      "ronaldo: 0.8490\n"
     ]
    }
   ],
   "source": [
    "context = [\"messi\", \"maradona\", \"neymar\", \"forlan\", \"aguero\"]\n",
    "predictions, used_words = cbow_predict(context, model)\n",
    "\n",
    "print(\"Used context words:\", used_words)\n",
    "print(\"Top predictions:\")\n",
    "for word, score in predictions:\n",
    "    if word not in used_words:\n",
    "        print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
